{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ebb373",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b94fc",
   "metadata": {},
   "source": [
    "- sometimes when the LLM does not have enough information, it produces hallucination. so, the most proper way is to give it enough information to work out the real answer and not produce hallucinated information to fill in the gaps. RAG helps fight hallucination.\n",
    "- RAG actually searches for information from a bunch of websites (runs a search, then augment the prompt with the search result information, and then generate the answer)\n",
    "- Sometimes, the RAG misses information even if it is there because the topic sentence (context) might be missing. that is why the RAG wont be able to understand whether it is relative information or not.\n",
    "- Giving focused information will reduce the chance to get vague answers to our questions\n",
    "- Even out of order information can sometimes make the RAG confused.\n",
    "- Under the hood: Text is converted to numbers (vectors). These numbers catch the symentic meaning behind assembled word that are close to each other. these meanings are then used for task generation. RAG searches similar embeddings and retrieve associated chunks to create the RAG prompt.\n",
    "- When achieving answer, tell it to \"insert line number supporting evidence\" to identify hallucination and fact-check of augmentation process."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
